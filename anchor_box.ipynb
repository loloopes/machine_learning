{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b80627e-0108-4184-8a8b-ca5e38b9b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "torch.set_printoptions(2) # Simplify printing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab5c30c-8dfa-4f8f-9448-69c0956fa03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def multibox_prior(data, sizes, ratios):\n",
    "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = torch.tensor(sizes, device=device)\n",
    "    ratio_tensor = torch.tensor(ratios, device=device)\n",
    "    # Offsets are required to move the anchor to the center of a pixel. Since\n",
    "    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height # Scaled steps in y axis\n",
    "    steps_w = 1.0 / in_width # Scaled steps in x axis\n",
    "    # Generate all center points for the anchor boxes\n",
    "    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n",
    "    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n",
    "    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n",
    "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
    "    # Generate `boxes_per_pixel` number of heights and widths that are later\n",
    "    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n",
    "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
    "                    sizes[0] * torch.sqrt(ratio_tensor[1:])))\n",
    "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
    "                    sizes[0] / torch.sqrt(ratio_tensor[1:])))\n",
    "    # Divide by 2 to get half height and half width\n",
    "    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(boxes_per_pixel, 1)\n",
    "    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n",
    "    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n",
    "    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n",
    "                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
    "    output = out_grid.unsqueeze(0) + anchor_manipulations.unsqueeze(1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9fe216f-23f1-410f-88ba-7c815fe0c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 1248480, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = d2l.plt.imread('../img/catdog-612x612.jpg')\n",
    "h, w = img.shape[:2]\n",
    "print(h, w)\n",
    "X = torch.rand(size=(1, 3, h, w)) # Construct input data\n",
    "Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9cceb5f-48bd-43e1-8bf5-b36ad8f8b035",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[408, 612, 5, 4]' is invalid for input of size 124848000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reshaped_Y \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m boxes[\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m0\u001b[39m, :]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[408, 612, 5, 4]' is invalid for input of size 124848000"
     ]
    }
   ],
   "source": [
    "reshaped_Y = Y.view(h, w, 5, 4)\n",
    "boxes[250, 250, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e5602-b077-40ce-8a66-d917025fd8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
